{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd637d2-826c-44ad-8fd3-8e79eec58537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install wordcloud\n",
    "# Install the latest version of the library\n",
    "!pip install --trusted-host nexus.aws-data-prd.dc-ifood.com --extra-index-url http://nexus.aws-data-prd.dc-ifood.com/repository/ifood-data-private-pypi/simple genplat-databricks-lib\n",
    "\n",
    "# Import necessary libraries\n",
    "from genplat_databricks_lib.setup import setup_databricks\n",
    "text_generation_udf = setup_databricks()\n",
    "from genplat_databricks_lib import text_generation\n",
    "from genplat_databricks_lib import apply_text_generation_to_dataframe\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, concat, udf, explode, split, rank,count, first\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ifood_databricks.gcp import gsheet\n",
    "from ifood_databricks.gcp.gsheet import gsheets_data_dump\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9ecfe5-b8ab-4620-975b-5dd064c78731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM people.qualtrics.qualtrics_individual_responses_hcm_avd_hash_view\n",
    "order by datereport desc\n",
    "\"\"\"\n",
    "\n",
    "fala_ai = spark.sql(query)\n",
    "fala_ai = fala_ai.filter((col('survey_received_at') >= '2024-01-01')&(col('datereport') < '2025-11-01')).orderBy('pseudo_person_id')\n",
    "fala_ai.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f69f9ae-e15f-4414-bc1e-044e3e5c9343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Based on all questions and score until current date_report, select last date_report questions that happened until that date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, add_months\n",
    "\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"pseudo_person_id\",'question')\n",
    "    .orderBy(col(\"datereport\").desc())\n",
    ")\n",
    "#enumerate questions \n",
    "exploded = fala_ai.withColumn(\"rn\", rank().over(window_spec)).orderBy( col(\"pseudo_person_id\"),col(\"rn\"),col(\"datereport\").desc())\n",
    "\n",
    "exploded = exploded.withColumn(\n",
    "    \"voluntary_turnover_one_month_flag\",\n",
    "    when(\n",
    "        (col(\"terminationaction\") == \"Voluntário\") &\n",
    "        (add_months(col(\"datereport\"), 1) <= col(\"terminationdate\")),\n",
    "        lit(1)\n",
    "    ).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "\n",
    "exploded.filter(col('pseudo_person_id')=='1000000031717ZA').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51497a4-2b8d-47ee-b92b-d142d16ded3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploded.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e713aa-2a59-45e8-8b6b-ccabdec3711f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, col\n",
    "columns_to_group = [\"pseudo_person_id\", \"datereport\",'n4_owner_area','n3_owner_area','n2_owner_area',\"voluntary_turnover_one_month_flag\",'terminationdate',\"score_before\",\"score_before_2\",\"grade\",\"race\",\"identidade_genero\",\"salary_basis\",'hcm_salary_amount_diff_abs','tempo_de_casa_em_dias','age']\n",
    "pivoted_exploded = (\n",
    "    exploded.groupBy(columns_to_group)\n",
    "    .pivot(\"question_id\")\n",
    "    .agg(first(\"score\"))\n",
    ").orderBy(\"datereport\")\n",
    "\n",
    "\n",
    "display(pivoted_exploded.filter(col('pseudo_person_id')=='1000000031717ZA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b88ff57-2550-4771-80e1-bb5119b9075a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "pivoted_exploded = (\n",
    "    exploded.groupBy(columns_to_group)\n",
    "    .pivot(\"question_id\")\n",
    "    .agg(F.first(\"score\"))\n",
    ").orderBy(\"datereport\")\n",
    "\n",
    "# janela: por pessoa, ordenado por data, olhando tudo até a linha atual\n",
    "w = (\n",
    "    Window\n",
    "    .partitionBy(\"pseudo_person_id\")\n",
    "    .orderBy(\"datereport\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "\n",
    "# identifica automaticamente as colunas de questões\n",
    "question_cols = [\n",
    "    c for c in pivoted_exploded.columns\n",
    "    if c not in [columns_to_group]\n",
    "]\n",
    "\n",
    "# aplica forward fill em cada coluna de questão\n",
    "filled = pivoted_exploded\n",
    "for col in question_cols:\n",
    "    filled = filled.withColumn(\n",
    "        col,\n",
    "        F.last(col, ignorenulls=True).over(w)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5b0896-1c67-4f56-b86b-6893f9e13154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filled.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09dcb682-89b9-4a87-a23c-b2d209ebb322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "filled.groupBy('score_before').count().display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfe7982-4725-4e54-bc22-1fb98f49c1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_avd = {\n",
    "  'Voando Alto': 5,\n",
    "  'Brilhou na Entrega': 4,\n",
    "  'Na Rota': 3,\n",
    "  'Ajustando a Rota': 2,\n",
    "  'Estacionado': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2eb17a-e164-4315-844b-406618d1559e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d772ae83-b42a-4516-8dd5-36d5f669cd5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "\n",
    "\n",
    "SALARY_BASIS_DICT = {\n",
    "    \"EXECUTIVE\": \"EXECUTIVE\",\n",
    "    \"DATA EXECUTIVE\": \"EXECUTIVE\",\n",
    "    \"DATA BI EXECUTIVE\": \"EXECUTIVE\",\n",
    "    \"PRODUTO & DESIGNER EXECUTIVE\": \"EXECUTIVE\",\n",
    "    \"TECH EXECUTIVE\": \"EXECUTIVE\",\n",
    "    \"PRODUTO & DESIGNER\": \"PRODUTO E DESIGN\",\n",
    "    \"DATA BI\": \"DATA BI\",\n",
    "    \"DATA\": \"DATA\",\n",
    "    \"NON-TECH\": \"NON-TECH\",\n",
    "    \"TECH\": \"TECH\",\n",
    "    \"IFOOD BENEFICIOS - SDR\": \"IFOOD BENEFICIOS\",\n",
    "    \"IFOOD BENEFICIOS - CANAIS\": \"IFOOD BENEFICIOS\",\n",
    "    \"IFOOD BENEFICIOS - ENGAJAMENTO\": \"IFOOD BENEFICIOS\",\n",
    "    \"IFOOD BENEFICIOS - KEY ACCOUNT\": \"IFOOD BENEFICIOS\",\n",
    "    \"IFOOD BENEFICIOS - CLOSER\": \"IFOOD BENEFICIOS\",\n",
    "    \"ADS - COORD COMERCIAL\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - SUPERVISORES\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - COORDENACAO\": \"COMERCIAL\",\n",
    "    \"IFOOD BENEFICIOS - COORD COMERCIAL\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - COORDENADORES\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - ZOOP\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - VENDAS EXTERNA\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - VENDAS INTERNA\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - KEY ACCOUNT\": \"COMERCIAL\",\n",
    "    \"COMERCIAL - POS VENDAS\": \"COMERCIAL\",\n",
    "    \"DATA_IFOOD FRANÇA\": \"GLOBAL MOBILITY\",\n",
    "    \"DATA_IFOOD PORTUGAL\": \"GLOBAL MOBILITY\",\n",
    "    \"EXECUTIVE TECH - IFOOD PORTUGAL\": \"GLOBAL MOBILITY\",\n",
    "    \"FRANÇA - TECH/DATA/PROD/DESIGN_EXEC\": \"GLOBAL MOBILITY\",\n",
    "    \"NON TECH - IFOOD PORTUGAL\": \"GLOBAL MOBILITY\",\n",
    "    \"NON-TECH - IFOOD COLOMBIA\": \"GLOBAL MOBILITY\",\n",
    "    \"PRODUTO & DESIGNER- IFOOD PORTUGAL\": \"GLOBAL MOBILITY\",\n",
    "    \"TECH - IFOOD PORTUGAL\": \"GLOBAL MOBILITY\",\n",
    "    \"TECH_IFOOD_FRANÇA\": \"GLOBAL MOBILITY\",\n",
    "}\n",
    "\n",
    "\n",
    "#adapting salary basis to the dict SALARY_BASIS_DICT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "salary_basis_map_udf = udf(lambda x: SALARY_BASIS_DICT.get(x, x) if x is not None else None, StringType())\n",
    "filled = filled.withColumn(\"salary_basis\", salary_basis_map_udf(\"salary_basis\"))\n",
    "filled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91a1b65-a98d-434b-83f5-0c73aa8e03bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#change score_before for dict_avd, if not in dict remove from df\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "filled = filled.withColumn(\n",
    "    \"sem_avaliacao\",\n",
    "    when(col(\"score_before\").isNull() | (col(\"score_before\") == 'Sem Avaliacao'), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "filled = filled.withColumn(\n",
    "    \"sem_avaliacao_anterior\",\n",
    "    when(col(\"score_before_2\").isNull() | (col(\"score_before_2\") == 'Sem Avaliacao'), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "filled = filled.withColumn(\n",
    "    \"score_before\",\n",
    "    when(col(\"score_before\") == 'Voando Alto', 5)\n",
    "    .when(col(\"score_before\") == 'Brilhou na Entrega', 4)\n",
    "    .when(col(\"score_before\") == 'Na Rota', 3)\n",
    "    .when(col(\"score_before\") == 'Ajustando a Rota', 2)\n",
    "    .when(col(\"score_before\") == 'Estacionado', 1)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "filled = filled.withColumn(\n",
    "    \"score_before_2\",\n",
    "    when(col(\"score_before_2\") == 'Voando Alto', 5)\n",
    "    .when(col(\"score_before_2\") == 'Brilhou na Entrega', 4)\n",
    "    .when(col(\"score_before_2\") == 'Na Rota', 3)\n",
    "    .when(col(\"score_before_2\") == 'Ajustando a Rota', 2)\n",
    "    .when(col(\"score_before_2\") == 'Estacionado', 1)\n",
    "    .otherwise(None)\n",
    ")\n",
    "#cria coluna sem avaliacao se null ou 'Sem Avaliacao'\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "\n",
    "\n",
    "filled = filled.filter(~((col(\"score_before\").isNull()) & (col(\"sem_avaliacao\") == 0)))\n",
    "\n",
    "filled = filled.filter(~((col(\"score_before_2\").isNull()) & (col(\"sem_avaliacao_anterior\") == 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6ef245-efd3-4d8c-b8ec-7074fec21f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filled.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f63acff-7484-4d0a-816e-68a82b160d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"race\", \"identidade_genero\", \"salary_basis\"]\n",
    "\n",
    "# Convert Spark DataFrame to pandas DataFrame\n",
    "wide_df_pd = filled.toPandas()\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "wide_df_ohe = pd.get_dummies(\n",
    "    wide_df_pd,\n",
    "    columns=cat_cols,\n",
    "    dummy_na=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a50155c2-b6d3-4bcb-a0d0-64cdaf79d833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wide_df_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7f091b-592a-4eaf-a1c9-18194d4f9906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save csv\n",
    "wide_df_ohe.to_csv(\"juliana_alves_turnover_pred_wide_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4eb887-8de2-44ab-86cd-7f070aa23b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af11d11f-c829-4c6b-bf7d-a3eedd0c2d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#spliting X and Y to predicit flag voluntary_turnover_one_month_flag\n",
    "X = wide_df_ohe.drop([\"voluntary_turnover_one_month_flag\"], axis=1)\n",
    "y = wide_df_ohe[\"voluntary_turnover_one_month_flag\"]\n",
    "\n",
    "# Drop or encode non-numeric columns\n",
    "# X_features = X.drop(['pseudo_person_id', 'datereport', 'terminationdate','n4_owner_area','n3_owner_area','n2_owner_area'], axis=1)\n",
    "X_features = X.set_index(['pseudo_person_id', 'datereport', 'terminationdate','n4_owner_area','n3_owner_area','n2_owner_area'])\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_features_imputed = imputer.fit_transform(X_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features_imputed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e7c171-86f6-4818-b154-aa9afb66990a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#feature importance\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X_features.columns\n",
    "feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "feature_importance_df.reset_index(drop=True, inplace=True)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0090e54c-6cdc-4758-8962-c9aae1e61c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list(feature_importance_df.Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d057345b-2d8a-4ac0-9a96-4d3280d16334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#plot feature importance - top 10\n",
    "top_10 = feature_importance_df.head(10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10[\"Feature\"], top_10[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4384f9b0-c752-4a68-b218-fd6fc3938e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#avaulatign acuracy, precision and recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19cf530a-6a9e-49b4-8618-35acea4a2101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b5abae-b979-4718-9ab1-92758e6665b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8eb7d2-3526-47f3-857c-b3b7b3636e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0452cb9d-4f91-4d2a-8e86-da086f0712f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#make a simple cross validation 5 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\"\n",
    "}\n",
    "\n",
    "results = cross_validate(\n",
    "    clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ccf28a9-99d1-4e98-a1f5-0f8f40204bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "display(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463fd683-36ed-41f0-91a1-a7f3b5e8367c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [5, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "#train model with best parameters\n",
    "best_clf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test)\n",
    "#avaulatign acuracy, precision and recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "individual turnover predictions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}